DOF: Moving along all directions
Timestep: 1e-3
Aerodynamics: Mujoco built-in
Reward: if terminated: if timestep < 1000: reward -= 10
Reward: Rich-Sutton reward - No
Reward: Penalize z-error, Extra penalty for x,y-error
Launch Control: if timestep < 100
I/O History: Short(4) / Obs (97,) - No
Action Bound: ctrl[6:] = [0, 0]

net_arch = {'pi': [64,64,64],
            'vf': [64,64,64]}

def linear_schedule(initial_value):
    if isinstance(initial_value, str):
        initial_value = float(initial_value)
    def func(progress):
        return progress * initial_value
    return func

model = PPO('MlpPolicy', 
            env=env,
            learning_rate=3e-4,
            n_steps=2048, # The number of steps to run for each environment per update / 2048
            batch_size=256,
            gamma=0.99,  # 0.99 # look forward 1.65s
            gae_lambda=0.95,
            clip_range=linear_schedule(0.2),
            ent_coef=0.01, # Makes PPO explore
            verbose=1,
            policy_kwargs={'net_arch':net_arch},
            tensorboard_log=log_path,
            device='mps')